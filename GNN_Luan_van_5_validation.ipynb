{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-74g4OWeZUhJ",
        "outputId": "a2dbdc33-be59-400f-90f6-fa3d0042fb9b"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5jT7Y1cZeDK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, LGConv\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import random, math\n",
        "from torch.utils.data import random_split\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbcXerNCJQW8"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZEkJm2zZhUH"
      },
      "outputs": [],
      "source": [
        "class RecommenderDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, interaction_file, batch_size=1204, val_size=0.3):\n",
        "        super().__init__()\n",
        "        self.interaction_file = interaction_file\n",
        "        self.batch_size = batch_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Load interactions\n",
        "        df = pd.read_csv(self.interaction_file, sep =\"\\t\")\n",
        "\n",
        "        # Unique users and items\n",
        "        unique_users = df['user_id:token'].unique()\n",
        "        unique_items = df['item_id:token'].unique()\n",
        "\n",
        "        self.num_users = len(unique_users)\n",
        "        self.num_items = len(unique_items)\n",
        "\n",
        "        # Mapping\n",
        "        self.user_to_idx = {u: idx for idx, u in enumerate(unique_users)}\n",
        "        self.item_to_idx = {i: idx for idx, i in enumerate(unique_items)}\n",
        "\n",
        "        # Build positive interactions\n",
        "        interactions = []\n",
        "        for _, row in df.iterrows():\n",
        "            u = self.user_to_idx[row['user_id:token']]\n",
        "            i = self.item_to_idx[row['item_id:token']]\n",
        "            interactions.append((u, i + self.num_users))\n",
        "\n",
        "        # Split interactions into train_dataset and val_dataset\n",
        "        dataset = interactions\n",
        "        train_size = int(len(dataset) * (1 - self.val_size))\n",
        "        val_size = len(dataset) - train_size\n",
        "        self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        # Build edge_index from train_dataset\n",
        "        train_user_ids = [u for u, _ in self.train_dataset]\n",
        "        train_item_ids = [i for _, i in self.train_dataset]\n",
        "\n",
        "        self.edge_index = torch.tensor([train_user_ids + train_item_ids, train_item_ids + train_user_ids], dtype=torch.long)\n",
        "\n",
        "        train_user_pos_items = defaultdict(set)\n",
        "        for u, i in self.train_dataset:\n",
        "            train_user_pos_items[u].add(i)\n",
        "\n",
        "        val_user_pos_items = defaultdict(set)\n",
        "        for u, i in self.val_dataset:\n",
        "            val_user_pos_items[u].add(i)\n",
        "\n",
        "        self.train_user_pos_items = train_user_pos_items\n",
        "        self.val_user_pos_items = val_user_pos_items\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsRgjkdugDEP"
      },
      "outputs": [],
      "source": [
        "class BaseRecommender(pl.LightningModule):\n",
        "    def __init__(self, num_users, num_items, embedding_dim=64, learning_rate=0.005, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_device = model_device\n",
        "\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.edge_index = None\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.edge_index = self.trainer.datamodule.edge_index.to(self.model_device)\n",
        "        self.train_user_pos_items = self.trainer.datamodule.train_user_pos_items\n",
        "        self.val_user_pos_items = self.trainer.datamodule.val_user_pos_items\n",
        "\n",
        "    @staticmethod\n",
        "    def hit_at_k(pred_items, true_items, k):\n",
        "        hits = 0\n",
        "        for pred, true in zip(pred_items, true_items):\n",
        "            if len(set(pred[:k]) & set(true)) > 0:\n",
        "                hits += 1\n",
        "        return hits / len(true_items)\n",
        "\n",
        "    @staticmethod\n",
        "    def ndcg_at_k(pred_items, true_items, k):\n",
        "        ndcg = 0.0\n",
        "        for pred, true in zip(pred_items, true_items):\n",
        "            gains = []\n",
        "            for idx, item in enumerate(pred[:k]):\n",
        "                gains.append(1 if item in true else 0)\n",
        "            ideal_gains = [1] * min(len(true), k)\n",
        "            dcg = sum(g / math.log2(i+2) for i, g in enumerate(gains))\n",
        "            idcg = sum(g / math.log2(i+2) for i, g in enumerate(ideal_gains))\n",
        "            ndcg += dcg / idcg if idcg > 0 else 0\n",
        "        return ndcg / len(true_items)\n",
        "\n",
        "    @staticmethod\n",
        "    def recall_at_k(pred_items, true_items, k):\n",
        "        recall = 0.0\n",
        "        for pred, true in zip(pred_items, true_items):\n",
        "            recall += len(set(pred[:k]) & set(true)) / len(true)\n",
        "        return recall / len(true_items)\n",
        "\n",
        "    @staticmethod\n",
        "    def precision_at_k(pred_items, true_items, k):\n",
        "        precision = 0.0\n",
        "        for pred, true in zip(pred_items, true_items):\n",
        "            precision += len(set(pred[:k]) & set(true)) / k\n",
        "        return precision / len(true_items)\n",
        "\n",
        "    def compute_loss(self, batch, full_user_embs, full_item_embs):\n",
        "        user_ids, item_ids = batch\n",
        "        pos_item_ids = item_ids - self.hparams.num_users\n",
        "\n",
        "        # Get embeddings\n",
        "        user_emb = full_user_embs[user_ids]\n",
        "        pos_emb = full_item_embs[pos_item_ids]\n",
        "\n",
        "        # Compute positive scores\n",
        "        pos_scores = torch.exp(-torch.abs(user_emb - pos_emb).sum(dim=1))\n",
        "\n",
        "        ####################### Hard negative Sampling #######################\n",
        "        distances = torch.cdist(user_emb, full_item_embs, p=1)\n",
        "        scores = torch.exp(-distances)\n",
        "\n",
        "        ######## Mask all pos_item_ids of the user in train_dataset ########\n",
        "        ### Basically, the  model should only see the information in the train_dataset.\n",
        "        ### Therefore, only mask the pos_item_ids of the user in train_dataset\n",
        "        ### All cell (user, item) in val_dataset should be treated as blank hence don't mask the val_dataset\n",
        "\n",
        "        for i, u in enumerate(user_ids.tolist()):\n",
        "            pos_item_ids = [item - self.num_users for item in self.train_user_pos_items[u]]\n",
        "            scores[i, pos_item_ids] = float('-inf')\n",
        "        ######## Mask all pos_item_ids of the user in train_dataset ########\n",
        "\n",
        "        k = 10 # Select top-K most negatives for each user\n",
        "        neg_item_ids = torch.topk(scores, k=k, dim=1).indices\n",
        "\n",
        "        # Get embeddings for these negatives\n",
        "        neg_emb = full_item_embs[neg_item_ids]\n",
        "\n",
        "        neg_scores = torch.exp(-torch.abs(user_emb.unsqueeze(1) - neg_emb).sum(dim=2))\n",
        "        neg_scores = neg_scores.mean(dim=1)\n",
        "        ####################### Hard negative Sampling #######################\n",
        "\n",
        "\n",
        "        ####################### Compute Loss #######################\n",
        "        scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
        "        labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)], dim=0)\n",
        "\n",
        "        loss = F.binary_cross_entropy(scores, labels)\n",
        "        ####################### Compute Loss #######################\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        full_user_embs, full_item_embs = self()\n",
        "        loss = self.compute_loss(batch, full_user_embs, full_item_embs)\n",
        "\n",
        "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        user_ids, item_ids = batch\n",
        "\n",
        "        full_user_embs, full_item_embs = self()\n",
        "        user_emb = full_user_embs[user_ids]\n",
        "\n",
        "        distances = torch.cdist(user_emb, full_item_embs, p=1)\n",
        "        scores = torch.exp(-distances)  # it is the score between the ith user in batch_size and ALL items\n",
        "\n",
        "        ########## Mask those user-item pair that already in training set so that it won't suggest again\n",
        "        mask = torch.zeros_like(scores, dtype=torch.bool)\n",
        "        for i, u in enumerate(user_ids.tolist()):\n",
        "            trained_items = [item - self.num_users for item in self.train_user_pos_items[u]]\n",
        "            mask[i, trained_items] = True\n",
        "\n",
        "        scores = scores.masked_fill(mask, float('-inf'))    #### Make them to -inf so that TopK won't pick again\n",
        "        ########## Mask those user-item pair that already in training set so that it won't suggest again\n",
        "\n",
        "\n",
        "        ################ Calculate metrics\n",
        "        k_values = [5, 10, 15, 20]  # Example: you can add more values as needed\n",
        "        # k_values = [10]\n",
        "\n",
        "        for k in k_values:\n",
        "            # Get top-k items for this k\n",
        "            topk_items = torch.topk(scores, k=k, dim=1).indices.tolist() # (1024, K=5)\n",
        "\n",
        "            true_items = []  # each user may have multiple positive items\n",
        "            for u in user_ids.tolist():\n",
        "                adjusted_val_items = [item - self.num_users for item in self.val_user_pos_items[u]]\n",
        "                true_items.append(adjusted_val_items)\n",
        "\n",
        "            # Compute metrics for this k\n",
        "            hit = self.hit_at_k(topk_items, true_items, k)\n",
        "            ndcg = self.ndcg_at_k(topk_items, true_items, k)\n",
        "            recall = self.recall_at_k(topk_items, true_items, k)\n",
        "            precision = self.precision_at_k(topk_items, true_items, k)\n",
        "\n",
        "            # Log metrics dynamically\n",
        "            self.log(f\"val_hit@{k:02d}\", hit, prog_bar=True)\n",
        "            self.log(f\"val_recall@{k:02d}\", recall, prog_bar=True)\n",
        "            self.log(f\"val_precision@{k:02d}\", precision, prog_bar=True)\n",
        "            self.log(f\"val_ndcg@{k:02d}\", ndcg, prog_bar=True)\n",
        "\n",
        "        # # Compute bpr_loss\n",
        "        # loss = self.compute_loss(batch, full_user_embs, full_item_embs)\n",
        "\n",
        "        # self.log('val_loss', loss, prog_bar=True, logger=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfUPqsyX54v9"
      },
      "outputs": [],
      "source": [
        "class GCNRecommender(BaseRecommender):\n",
        "    def __init__(self, num_users, num_items, embedding_dim=64,hidden_dim=64, learning_rate=0.005, dropout=0.2):\n",
        "\n",
        "        super().__init__(num_users, num_items, embedding_dim, learning_rate, dropout)\n",
        "\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim).to(self.model_device)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim).to(self.model_device)\n",
        "\n",
        "        self.conv1 = GCNConv(embedding_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self):\n",
        "        # Initial embeddings for all nodes\n",
        "        full_user_embs = self.user_embedding.weight.to(self.model_device)\n",
        "        full_item_embs = self.item_embedding.weight.to(self.model_device)\n",
        "        x = torch.cat([full_user_embs, full_item_embs], dim=0)\n",
        "\n",
        "        # GCN propagation over full graph\n",
        "        x = F.relu(self.conv1(x, self.edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, self.edge_index)\n",
        "\n",
        "        # Split back into user/item embeddings\n",
        "        user_final = x[:self.num_users]\n",
        "        item_final = x[self.num_users:]\n",
        "        return user_final, item_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUnhr4GBZsl3"
      },
      "outputs": [],
      "source": [
        "class GATRecommender(BaseRecommender):\n",
        "    def __init__(self, num_users, num_items, embedding_dim=64, hidden_dim=64,\n",
        "                 heads=8, learning_rate=0.005, dropout=0.2):\n",
        "        super().__init__(num_users, num_items, embedding_dim, learning_rate, dropout)\n",
        "\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim).to(self.model_device)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim).to(self.model_device)\n",
        "        self.heads = heads\n",
        "\n",
        "        self.conv1 = GATConv(embedding_dim, hidden_dim, heads=heads, dropout=dropout)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=dropout)\n",
        "\n",
        "    def forward(self):\n",
        "        # Initial embeddings for all nodes\n",
        "        full_user_embs = self.user_embedding.weight.to(self.model_device)\n",
        "        full_item_embs = self.item_embedding.weight.to(self.model_device)\n",
        "        x = torch.cat([full_user_embs, full_item_embs], dim=0)\n",
        "\n",
        "        # GCN propagation over full graph\n",
        "        x = F.relu(self.conv1(x, self.edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, self.edge_index)\n",
        "\n",
        "        # Split back into user/item embeddings\n",
        "        user_final = x[:self.num_users]\n",
        "        item_final = x[self.num_users:]\n",
        "        return user_final, item_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi-XkzOFZvCV"
      },
      "outputs": [],
      "source": [
        "class LightGCNRecommender(BaseRecommender):\n",
        "    def __init__(self, num_users, num_items, embedding_dim=64, num_layers=8,\n",
        "                 learning_rate=0.005, dropout=0.2):\n",
        "        super().__init__(num_users, num_items, embedding_dim, learning_rate, dropout)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim).to(self.model_device)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim).to(self.model_device)\n",
        "\n",
        "        self.convs = nn.ModuleList([LGConv() for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self):\n",
        "        full_user_embs = self.user_embedding.weight.to(self.model_device)\n",
        "        full_item_embs = self.item_embedding.weight.to(self.model_device)\n",
        "        x = torch.cat([full_user_embs, full_item_embs], dim=0)\n",
        "\n",
        "        embeddings = [x]\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, self.edge_index)\n",
        "            embeddings.append(x)\n",
        "\n",
        "        # Combine all layers with equal weight\n",
        "        final_embeddings = torch.stack(embeddings, dim=1).mean(dim=1)\n",
        "\n",
        "        user_final = final_embeddings[:self.num_users]\n",
        "        item_final = final_embeddings[self.num_users:]\n",
        "\n",
        "        return user_final, item_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DV4_3bRZxuH"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     data_module = RecommenderDataModule('Amazon-KG-5core-Books.inter')\n",
        "#     data_module.prepare_data()\n",
        "\n",
        "#     models = {\n",
        "#         # 'GAT': GATRecommender(\n",
        "#         #     num_users=data_module.num_users,\n",
        "#         #     num_items=data_module.num_items,\n",
        "#         #     embedding_dim=128,\n",
        "#         #     hidden_dim=128,\n",
        "#         #     heads=8,\n",
        "#         #     learning_rate=0.005\n",
        "#         # ),\n",
        "#         'GCN': GCNRecommender(\n",
        "#             num_users=data_module.num_users,\n",
        "#             num_items=data_module.num_items,\n",
        "#             embedding_dim=128,\n",
        "#             hidden_dim=128,\n",
        "#             learning_rate=0.005\n",
        "#         ),\n",
        "#         'LightGCN': LightGCNRecommender(\n",
        "#             num_users=data_module.num_users,\n",
        "#             num_items=data_module.num_items,\n",
        "#             embedding_dim=128,\n",
        "#             num_layers=8,\n",
        "#             learning_rate=0.005\n",
        "#         )\n",
        "#     }\n",
        "\n",
        "#     for model_name, model in models.items():\n",
        "#         print(f\"\\n{'='*60}\")\n",
        "#         print(f\"Training {model_name}\")\n",
        "#         print(f\"{'='*60}\")\n",
        "\n",
        "#         # Early stopping callback\n",
        "#         early_stop_callback = EarlyStopping(\n",
        "#             monitor=\"val_hit@10\",     # metric to monitor\n",
        "#             patience=20,            # number of epochs with no improvement after which training will be stopped\n",
        "#             mode=\"max\",             # 'min' because we want to minimize val_loss\n",
        "#             verbose=True\n",
        "#         )\n",
        "\n",
        "#         checkpoint_hit = ModelCheckpoint(\n",
        "#                 monitor=\"val_hit@10\",\n",
        "#                 dirpath=\"./checkpoints\",\n",
        "#                 filename=f\"{model_name}-'book'-{{epoch:02d}}-{{val_hit@10:.3f}}\",\n",
        "#                 save_top_k=1,\n",
        "#                 mode=\"max\",\n",
        "#         )\n",
        "\n",
        "#         checkpoint_recall = ModelCheckpoint(\n",
        "#                 monitor=\"val_recall@10\",\n",
        "#                 dirpath=\"./checkpoints\",\n",
        "#                 filename=f\"{model_name}-'book'-{{epoch:02d}}-{{val_recall@10:.3f}}\",\n",
        "#                 save_top_k=1,\n",
        "#                 mode=\"max\",\n",
        "#         )\n",
        "\n",
        "#         checkpoint_ndcg = ModelCheckpoint(\n",
        "#                 monitor=\"val_ndcg@10\",\n",
        "#                 dirpath=\"./checkpoints\",\n",
        "#                 filename=f\"{model_name}-'book'-{{epoch:02d}}-{{val_ndcg@10:.3f}}\",\n",
        "#                 save_top_k=1,\n",
        "#                 mode=\"max\",\n",
        "#         )\n",
        "\n",
        "#         checkpoint_precision = ModelCheckpoint(\n",
        "#                 monitor=\"val_precision@10\",\n",
        "#                 dirpath=\"./checkpoints\",\n",
        "#                 filename=f\"{model_name}-'book'-{{epoch:02d}}-{{val_precision@10:.3f}}\",\n",
        "#                 save_top_k=1,\n",
        "#                 mode=\"max\",\n",
        "#         )\n",
        "\n",
        "#         trainer = Trainer(\n",
        "#                 num_sanity_val_steps=0,\n",
        "#                 max_epochs=500,\n",
        "#                 accelerator=\"auto\",\n",
        "#                 callbacks=[checkpoint_hit, checkpoint_recall, checkpoint_ndcg, checkpoint_precision, early_stop_callback],\n",
        "#             )\n",
        "\n",
        "#         trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUCgflLYse2P"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720,
          "referenced_widgets": [
            "07825e330aff4e7ba397a7c99f3a00e0",
            "cacc82d517e84eefac813b840e460f8e"
          ]
        },
        "id": "Wmv79Q67ADpi",
        "outputId": "158b6f15-e630-447e-a058-0b239555f8e8"
      },
      "outputs": [],
      "source": [
        "# # ########### Validation\n",
        "data_module = RecommenderDataModule('Amazon-KG-5core-Books.inter')\n",
        "data_module.prepare_data()\n",
        "\n",
        "checkpoint_path = \"/content/checkpoints/GAT-'book'-epoch=37-val_ndcg@10=0.066.ckpt\"\n",
        "model = GATRecommender.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
        "trainer.validate(model, datamodule=data_module)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07825e330aff4e7ba397a7c99f3a00e0": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_cacc82d517e84eefac813b840e460f8e",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Validation <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 29/29 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:06 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">4.58it/s</span>  \n</pre>\n",
                  "text/plain": "Validation \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 29/29 \u001b[2m0:00:06 • 0:00:00\u001b[0m \u001b[2;4m4.58it/s\u001b[0m  \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "cacc82d517e84eefac813b840e460f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
