{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-5Jpgg49wzb",
    "outputId": "ec14c399-bd0d-4523-cdbc-4084ddfbfcfb"
   },
   "outputs": [],
   "source": [
    "# # 1. Gá»¡ bá» phiÃªn báº£n quÃ¡ má»›i hiá»‡n táº¡i\n",
    "# !pip uninstall torch torchvision torchaudio torch-scatter torch-sparse torch-geometric torch-geometric-temporal -y\n",
    "\n",
    "# # 2. CÃ i Ä‘áº·t PyTorch 2.5.1 (Báº£n á»•n Ä‘á»‹nh) + CUDA 12.4\n",
    "# !pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "# # 3. CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n vá»‡ tinh (Scatter/Sparse) dÃ nh RIÃŠNG cho báº£n 2.5.1\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
    "\n",
    "# # 4. CÃ i thÆ° viá»‡n chÃ­nh\n",
    "# !pip install pytorch_lightning torch-geometric torch-geometric-temporal\n",
    "\n",
    "# # # 5. Runtime > Restart session\n",
    "# # # 6 Ignore this !pip section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 6172,
     "status": "error",
     "timestamp": 1768380494093,
     "user": {
      "displayName": "van hao truong",
      "userId": "08998182992862504611"
     },
     "user_tz": -420
    },
    "id": "xDk5XgSY3-_w",
    "outputId": "4b24502f-92da-42d7-8a99-05db6574f795"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric_temporal.nn.recurrent import TGCN, EvolveGCNH\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set environment variables for reproducibility and safety\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 1. Configuration & Seeding\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, interaction_file, batch_size=1024, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "        super().__init__()\n",
    "        self.interaction_file = interaction_file\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # --- 1. Load & Preprocess ---\n",
    "        df = pd.read_csv(self.interaction_file)\n",
    "        \n",
    "        # Chuyá»ƒn timestamp\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values('timestamp')\n",
    "\n",
    "        df['month'] = df['timestamp'].dt.to_period('M')\n",
    "\n",
    "        # Mapping ID sang Index (0 -> N-1)\n",
    "        unique_users = df['user_id'].unique()\n",
    "        unique_items = df['item_id'].unique()\n",
    "        \n",
    "        self.num_users = len(unique_users)\n",
    "        self.num_items = len(unique_items)\n",
    "\n",
    "        self.user_to_idx = {u: idx for idx, u in enumerate(unique_users)}\n",
    "        self.item_to_idx = {i: idx for idx, i in enumerate(unique_items)}\n",
    "\n",
    "        # Ãp dá»¥ng mapping vÃ o DataFrame (Nhanh hÆ¡n iterrows ráº¥t nhiá»u)\n",
    "        df['user_idx'] = df['user_id'].map(self.user_to_idx)\n",
    "        df['item_idx'] = df['item_id'].map(self.item_to_idx)\n",
    "\n",
    "        # --- 2. Temporal Split ---\n",
    "        months = sorted(df['month'].unique())\n",
    "        n_months = len(months)\n",
    "        \n",
    "        train_end = int(n_months * self.train_size)\n",
    "        val_end = train_end + int(n_months * self.val_size)\n",
    "        \n",
    "        train_months = months[:train_end]\n",
    "        val_months = months[train_end:val_end]\n",
    "        test_months = months[val_end:]\n",
    "\n",
    "        # TÃ¡ch DataFrame\n",
    "        self.train_df = df[df['month'].isin(train_months)].sort_values('timestamp')\n",
    "        self.val_df = df[df['month'].isin(val_months)].sort_values('timestamp')\n",
    "        self.test_df = df[df['month'].isin(test_months)].sort_values('timestamp')\n",
    "\n",
    "        print(f\"Split sizes -> Train: {len(self.train_df)}, Val: {len(self.val_df)}, Test: {len(self.test_df)}\")\n",
    "\n",
    "        # --- 3. Build Graph (Edge Index) cho Train Set ---\n",
    "        # Chá»‰ dÃ¹ng dá»¯ liá»‡u Train Ä‘á»ƒ xÃ¢y dá»±ng Ä‘á»“ thá»‹ ná»n táº£ng\n",
    "        # Item nodes sáº½ cÃ³ ID tá»« num_users Ä‘áº¿n num_users + num_items - 1\n",
    "        \n",
    "        self.edge_index_all = []\n",
    "        \n",
    "        for month, group in df.groupby('month', sort=True):\n",
    "            # Láº¥y src (user) vÃ  dst (item)\n",
    "            # Chuyá»ƒn vá» Tensor ngay láº­p tá»©c Ä‘á»ƒ tiáº¿t kiá»‡m RAM\n",
    "            src = torch.tensor(group['user_idx'].values, dtype=torch.long)\n",
    "            dst = torch.tensor(group['item_idx'].values, dtype=torch.long) + self.num_users\n",
    "        \n",
    "            # Táº¡o edge_index vÃ´ hÆ°á»›ng (2 chiá»u: user->item vÃ  item->user)\n",
    "            edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n",
    "            self.edge_index_all.append(edge_index)\n",
    "\n",
    "        # --- 4. Prepare User History (Cho viá»‡c sampling/evaluation náº¿u cáº§n) ---\n",
    "        # DÃ¹ng set Ä‘á»ƒ tra cá»©u nhanh O(1)\n",
    "        self.train_user_pos_items = self._build_user_history(self.train_df)\n",
    "        self.val_user_pos_items = self._build_user_history(self.val_df)\n",
    "        self.test_user_pos_items = self._build_user_history(self.test_df)\n",
    "\n",
    "    def _build_user_history(self, df_subset):\n",
    "        \"\"\"HÃ m phá»¥ trá»£ Ä‘á»ƒ gom nhÃ³m item theo user\"\"\"\n",
    "        user_pos_items = defaultdict(set)\n",
    "        # Zip nhanh hÆ¡n iterrows\n",
    "        for u, i in zip(df_subset['user_idx'], df_subset['item_idx']):\n",
    "            user_pos_items[u].add(i)\n",
    "        return user_pos_items\n",
    "\n",
    "    def _create_dataloader(self, df_subset, shuffle):\n",
    "        # Chuyá»ƒn Ä‘á»•i thÃ nh TensorDataset Ä‘á»ƒ DataLoader hiá»ƒu\n",
    "        users = torch.tensor(df_subset['user_idx'].values, dtype=torch.long)\n",
    "        items = torch.tensor(df_subset['item_idx'].values, dtype=torch.long)\n",
    "        dataset = TensorDataset(users, items)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Shuffle=True cho Train set\n",
    "        return self._create_dataloader(self.train_df, shuffle=True) ## CÃ³ nÃªn shuffle khÃ´ng?\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._create_dataloader(self.val_df, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._create_dataloader(self.test_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8-L86-kxBGia"
   },
   "outputs": [],
   "source": [
    "class TGCNRecommender(pl.LightningModule):\n",
    "    def __init__(self, num_users, num_items, sequence_length, embedding_dim, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_device = model_device\n",
    "\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_nodes = self.num_users + self.num_items\n",
    "\n",
    "        self.seq_len = sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lr = lr\n",
    "\n",
    "        # Learnable Node Embeddings\n",
    "        self.node_emb = nn.Embedding(self.num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.node_emb.weight)\n",
    "\n",
    "        # T-GCN Layer\n",
    "        self.tgcn = TGCN(in_channels=embedding_dim, out_channels=embedding_dim) \n",
    "        self.h = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.edge_index_all = self.trainer.datamodule.edge_index_all\n",
    "\n",
    "        self.train_user_pos_items = self.trainer.datamodule.train_user_pos_items\n",
    "        self.val_user_pos_items = self.trainer.datamodule.val_user_pos_items\n",
    "        self.test_user_pos_items = self.trainer.datamodule.test_user_pos_items\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.h = None\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.h = None\n",
    "\n",
    "    def forward(self, edge_index_window):\n",
    "        # 1. Get current node embeddings\n",
    "        x = self.node_emb.weight    # [num_nodes, embedding_dim] ([18417, 64])\n",
    "\n",
    "        # 2. Update Embeddings with T-GCN\n",
    "        for edge_index in edge_index_window:\n",
    "            self.h = self.tgcn(x, edge_index, self.h) #h_out shape: [num_nodes, embedding_dim] ([18417, 64])\n",
    "\n",
    "        user_embs = self.h[:self.num_users]\n",
    "        item_embs = self.h[self.num_users:]\n",
    "\n",
    "        return user_embs, item_embs\n",
    "\n",
    "    def compute_loss(self, batch, user_embs, item_embs):\n",
    "        user_ids, item_ids = batch\n",
    "        pos_item_ids = item_ids\n",
    "\n",
    "        # Get embeddings\n",
    "        user_emb = user_embs[user_ids]\n",
    "        pos_emb = item_embs[pos_item_ids]\n",
    "\n",
    "        # Compute positive scores\n",
    "        pos_scores = torch.exp(-torch.abs(user_emb - pos_emb).sum(dim=1))\n",
    "\n",
    "        ####################### Hard negative Sampling #######################\n",
    "        distances = torch.cdist(user_emb, item_embs, p=1)\n",
    "        scores = torch.exp(-distances)\n",
    "\n",
    "        ######## Mask all pos_item_ids of the user in train_dataset ########\n",
    "        ### Basically, the  model should only see the information in the train_dataset.\n",
    "        ### Therefore, only mask the pos_item_ids of the user in train_dataset\n",
    "        ### All cell (user, item) in val_dataset should be treated as blank hence don't mask the val_dataset\n",
    "\n",
    "        for i, u in enumerate(user_ids.tolist()):\n",
    "            pos_item_ids = [item for item in self.train_user_pos_items[u]]\n",
    "            scores[i, pos_item_ids] = float('-inf')\n",
    "        ######## Mask all pos_item_ids of the user in train_dataset ########\n",
    "\n",
    "        k = 10 # Select top-K most negatives for each user\n",
    "        neg_item_ids = torch.topk(scores, k=k, dim=1).indices\n",
    "\n",
    "        # Get embeddings for these negatives\n",
    "        neg_emb = item_embs[neg_item_ids]\n",
    "\n",
    "        neg_scores = torch.exp(-torch.abs(user_emb.unsqueeze(1) - neg_emb).sum(dim=2))\n",
    "        neg_scores = neg_scores.mean(dim=1)\n",
    "        ####################### Hard negative Sampling #######################\n",
    "\n",
    "\n",
    "        ####################### Compute Loss #######################\n",
    "        scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
    "        labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)], dim=0)\n",
    "\n",
    "        loss = F.binary_cross_entropy(scores, labels)\n",
    "        ####################### Compute Loss #######################\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        for t in range(len(self.edge_index_all)):\n",
    "            edge_index_window = self.edge_index_all[t:t + self.seq_len]\n",
    "        \n",
    "            user_embs, item_embs = self(edge_index_window)\n",
    "            loss = self.compute_loss(batch, user_embs, item_embs)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        user_ids, item_ids = batch\n",
    "\n",
    "        user_embs, item_embs = self()\n",
    "        user_emb = full_user_embs[user_ids]\n",
    "\n",
    "        distances = torch.cdist(user_emb, full_item_embs, p=1)\n",
    "        scores = torch.exp(-distances)  # it is the score between the ith user in batch_size and ALL items\n",
    "\n",
    "        ########## Mask those user-item pair that already in training set so that it won't suggest again\n",
    "        mask = torch.zeros_like(scores, dtype=torch.bool)\n",
    "        for i, u in enumerate(user_ids.tolist()):\n",
    "            trained_items = [item for item in self.train_user_pos_items[u]]\n",
    "            mask[i, trained_items] = True\n",
    "\n",
    "        scores = scores.masked_fill(mask, float('-inf'))    #### Make them to -inf so that TopK won't pick again\n",
    "        ########## Mask those user-item pair that already in training set so that it won't suggest again\n",
    "\n",
    "\n",
    "        ################ Calculate metrics\n",
    "        # k_values = [5, 10, 15, 20]  # Example: you can add more values as needed\n",
    "        k_values = [10]\n",
    "\n",
    "        for k in k_values:\n",
    "            # Get top-k items for this k\n",
    "            topk_items = torch.topk(scores, k=k, dim=1).indices.tolist() # (1024, K=5)\n",
    "\n",
    "            true_items = []  # each user may have multiple positive items\n",
    "            for u in user_ids.tolist():\n",
    "                adjusted_val_items = [item - self.num_users for item in self.val_user_pos_items[u]]\n",
    "                true_items.append(adjusted_val_items)\n",
    "\n",
    "            # Compute metrics for this k\n",
    "            hit = self.hit_at_k(topk_items, true_items, k)\n",
    "            ndcg = self.ndcg_at_k(topk_items, true_items, k)\n",
    "            recall = self.recall_at_k(topk_items, true_items, k)\n",
    "            precision = self.precision_at_k(topk_items, true_items, k)\n",
    "\n",
    "            # Log metrics dynamically\n",
    "            self.log(f\"val_hit@{k:02d}\", hit, prog_bar=True)\n",
    "            self.log(f\"val_recall@{k:02d}\", recall, prog_bar=True)\n",
    "            self.log(f\"val_precision@{k:02d}\", precision, prog_bar=True)\n",
    "            self.log(f\"val_ndcg@{k:02d}\", ndcg, prog_bar=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "aRuOnaapBK81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes -> Train: 69843, Val: 36616, Test: 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type      | Params | Mode  | FLOPs\n",
      "-------------------------------------------------------\n",
      "0 | node_emb | Embedding | 1.2 M  | train | 0    \n",
      "1 | tgcn     | TGCN      | 37.2 K | train | 0    \n",
      "-------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.864     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes -> Train: 69843, Val: 36616, Test: 131\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TGCNRecommender.forward() missing 1 required positional argument: 'edge_index_window'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      5\u001b[39m model = TGCNRecommender(\n\u001b[32m      6\u001b[39m     num_users=data_module.num_users,\n\u001b[32m      7\u001b[39m     num_items=data_module.num_items,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     lr = \u001b[32m0.001\u001b[39m,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m trainer = pl.Trainer(\n\u001b[32m     14\u001b[39m     max_epochs=\u001b[32m10\u001b[39m,\n\u001b[32m     15\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     log_every_n_steps=\u001b[32m1\u001b[39m\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCompleted\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:584\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:630\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    623\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    624\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    626\u001b[39m     ckpt_path,\n\u001b[32m    627\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    628\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    629\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1079\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path, weights_only)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1076\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1077\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1078\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1083\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1084\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1121\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1123\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1150\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1147\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1149\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1152\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1154\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:146\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    145\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:441\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    435\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    436\u001b[39m step_args = (\n\u001b[32m    437\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    440\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:329\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    332\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mTGCNRecommender.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[32m    106\u001b[39m     user_ids, item_ids = batch\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     user_embs, item_embs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     user_emb = full_user_embs[user_ids]\n\u001b[32m    111\u001b[39m     distances = torch.cdist(user_emb, full_item_embs, p=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: TGCNRecommender.forward() missing 1 required positional argument: 'edge_index_window'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_module = DataModule('data/book_interaction.csv')\n",
    "    data_module.prepare_data()\n",
    "\n",
    "    model = TGCNRecommender(\n",
    "        num_users=data_module.num_users,\n",
    "        num_items=data_module.num_items,\n",
    "        sequence_length = 5,\n",
    "        embedding_dim= 64,\n",
    "        lr = 0.001,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=1\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, data_module)\n",
    "    print(\"Completed\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqa2xQZFwDaTImHhVca8F7",
   "gpuType": "T4",
   "mount_file_id": "1xEeIB3zDaFVJwvNavB_5aFrzl6IqQUC5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
