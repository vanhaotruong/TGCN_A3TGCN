{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-5Jpgg49wzb",
    "outputId": "ec14c399-bd0d-4523-cdbc-4084ddfbfcfb"
   },
   "outputs": [],
   "source": [
    "# # 1. Gỡ bỏ phiên bản quá mới hiện tại\n",
    "# !pip uninstall torch torchvision torchaudio torch-scatter torch-sparse torch-geometric torch-geometric-temporal -y\n",
    "\n",
    "# # 2. Cài đặt PyTorch 2.5.1 (Bản ổn định) + CUDA 12.4\n",
    "# !pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "# # 3. Cài đặt các thư viện vệ tinh (Scatter/Sparse) dành RIÊNG cho bản 2.5.1\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
    "\n",
    "# # 4. Cài thư viện chính\n",
    "# !pip install pytorch_lightning torch-geometric torch-geometric-temporal\n",
    "\n",
    "# # # 5. Runtime > Restart session\n",
    "# # # 6 Ignore this !pip section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 6172,
     "status": "error",
     "timestamp": 1768380494093,
     "user": {
      "displayName": "van hao truong",
      "userId": "08998182992862504611"
     },
     "user_tz": -420
    },
    "id": "xDk5XgSY3-_w",
    "outputId": "4b24502f-92da-42d7-8a99-05db6574f795"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric_temporal.nn.recurrent import TGCN\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set environment variables for reproducibility and safety\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 1. Configuration & Seeding\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrrEyokS4NK9"
   },
   "outputs": [],
   "source": [
    "def load_tgcn_data(data_dir='data'):\n",
    "    print(\"Loading interactions...\")\n",
    "    # Load and process interaction data\n",
    "    # Only using book_interaction.csv\n",
    "    file_path = os.path.join(data_dir, 'book_interaction.csv')\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    df_inter = pd.read_csv(file_path)\n",
    "    # Clean column names (strip type suffix if present, e.g. user_id:token -> user_id)\n",
    "    df_inter.columns = [c.split(':')[0] for c in df_inter.columns]\n",
    "\n",
    "    # Ensure timestamp is datetime\n",
    "    df_inter['timestamp'] = pd.to_datetime(df_inter['timestamp'])\n",
    "\n",
    "    print(\"Mapping IDs...\")\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "\n",
    "    # Encode Users and Items\n",
    "    df_inter['user_idx'] = user_encoder.fit_transform(df_inter['user_id'].astype(str))\n",
    "    df_inter['item_idx'] = item_encoder.fit_transform(df_inter['item_id'].astype(str))\n",
    "\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "\n",
    "    print(f\"Total Users: {num_users}, Total Items: {num_items}\")\n",
    "\n",
    "    print(\"Creating temporal snapshots...\")\n",
    "    df_inter['month'] = df_inter['timestamp'].dt.to_period('M')\n",
    "    # Sort by month to ensure temporal order\n",
    "    months = sorted(df_inter['month'].unique())\n",
    "\n",
    "    # 1. Determine Training Split (70%) to build the static graph\n",
    "    train_len_months = int(len(months) * 0.7)\n",
    "    train_months = months[:train_len_months]\n",
    "\n",
    "    # 2. Get all interactions in the training set\n",
    "    df_train = df_inter[df_inter['month'].isin(train_months)]\n",
    "\n",
    "    # 3. Build Static Edge Index from Train Set\n",
    "    train_u_idx = df_train['user_idx'].values\n",
    "    train_i_idx = df_train['item_idx'].values\n",
    "\n",
    "    train_u_node_idx = torch.tensor(train_u_idx, dtype=torch.long)\n",
    "    train_i_node_idx = torch.tensor(train_i_idx + num_users, dtype=torch.long)\n",
    "\n",
    "    # Create undirected graph from unique training interactions\n",
    "    # Note: torch.unique might be needed if multiple interactions exist, but edge_index usually works fine with multis.\n",
    "    # We'll use the raw list; duplicates increase weight in message passing or are redundant.\n",
    "    # For efficiency and cleanliness, let's keep unique edges.\n",
    "    train_edges_df = df_train[['user_idx', 'item_idx']].drop_duplicates()\n",
    "    unique_u_idx = torch.tensor(train_edges_df['user_idx'].values, dtype=torch.long)\n",
    "    unique_i_idx = torch.tensor(train_edges_df['item_idx'].values + num_users, dtype=torch.long)\n",
    "\n",
    "    train_edge_index = torch.stack([\n",
    "        torch.cat([unique_u_idx, unique_i_idx]),\n",
    "        torch.cat([unique_i_idx, unique_u_idx])\n",
    "    ], dim=0)\n",
    "\n",
    "    print(f\"Static Training Graph created with {train_edges_df.shape[0]} edges.\")\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for m in months:\n",
    "        snapshot_df = df_inter[df_inter['month'] == m]\n",
    "        if snapshot_df.empty:\n",
    "            continue\n",
    "\n",
    "        u_idx_raw = snapshot_df['user_idx'].values\n",
    "        i_idx_raw = snapshot_df['item_idx'].values\n",
    "\n",
    "        # Node indices for validataion/testing targets\n",
    "        u_node_idx = torch.tensor(u_idx_raw, dtype=torch.long)\n",
    "        i_node_idx = torch.tensor(i_idx_raw + num_users, dtype=torch.long)\n",
    "\n",
    "        # Use the STATIC train_edge_index for Graph Structure\n",
    "        dataset.append({\n",
    "            'edge_index': train_edge_index,\n",
    "            'y': torch.ones(len(snapshot_df), dtype=torch.float), # All interactions are positive (likes)\n",
    "            'target_u': u_node_idx,\n",
    "            'target_i': i_node_idx\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(dataset)} snapshots.\")\n",
    "\n",
    "    # Split into Train (First 70%) and Remainder (30%)\n",
    "    # Requirement: First 70% for training (temporal). Remaining 30% randomly split into Val and Test.\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(total_len * 0.7)\n",
    "\n",
    "    train_dataset = dataset[:train_len]\n",
    "    remainder_dataset = dataset[train_len:]\n",
    "\n",
    "    # Shuffle the remainder to randomize validation/test split\n",
    "    import random\n",
    "    random.shuffle(remainder_dataset)\n",
    "\n",
    "    # Allocate roughly 10% of total (1/3 of remainder) to val, and 20% (2/3 of remainder) to test\n",
    "    val_len = int(total_len * 0.1)\n",
    "\n",
    "    val_dataset = remainder_dataset[:val_len]\n",
    "    test_dataset = remainder_dataset[val_len:]\n",
    "\n",
    "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, interaction_file, batch_size=1024, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "        super().__init__()\n",
    "        self.interaction_file = interaction_file\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # --- 1. Load & Preprocess ---\n",
    "        df = pd.read_csv(self.interaction_file)\n",
    "        \n",
    "        # Chuyển timestamp\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df['month'] = df['timestamp'].dt.to_period('M')\n",
    "\n",
    "        # Mapping ID sang Index (0 -> N-1)\n",
    "        unique_users = df['user_id'].unique()\n",
    "        unique_items = df['item_id'].unique()\n",
    "        \n",
    "        self.num_users = len(unique_users)\n",
    "        self.num_items = len(unique_items)\n",
    "\n",
    "        self.user_to_idx = {u: idx for idx, u in enumerate(unique_users)}\n",
    "        self.item_to_idx = {i: idx for idx, i in enumerate(unique_items)}\n",
    "\n",
    "        # Áp dụng mapping vào DataFrame (Nhanh hơn iterrows rất nhiều)\n",
    "        df['user_idx'] = df['user_id'].map(self.user_to_idx)\n",
    "        df['item_idx'] = df['item_id'].map(self.item_to_idx)\n",
    "\n",
    "        # --- 2. Temporal Split ---\n",
    "        months = sorted(df['month'].unique())\n",
    "        n_months = len(months)\n",
    "        \n",
    "        train_end = int(n_months * self.train_size)\n",
    "        val_end = train_end + int(n_months * self.val_size)\n",
    "        \n",
    "        train_months = months[:train_end]\n",
    "        val_months = months[train_end:val_end]\n",
    "        test_months = months[val_end:]\n",
    "\n",
    "        # Tách DataFrame\n",
    "        self.train_df = df[df['month'].isin(train_months)]\n",
    "        self.val_df = df[df['month'].isin(val_months)]\n",
    "        self.test_df = df[df['month'].isin(test_months)]\n",
    "\n",
    "        print(f\"Split sizes -> Train: {len(self.train_df)}, Val: {len(self.val_df)}, Test: {len(self.test_df)}\")\n",
    "\n",
    "        # --- 3. Build Graph (Edge Index) cho Train Set ---\n",
    "        # Chỉ dùng dữ liệu Train để xây dựng đồ thị nền tảng\n",
    "        # Item nodes sẽ có ID từ num_users đến num_users + num_items - 1\n",
    "        src = torch.tensor(self.train_df['user_idx'].values, dtype=torch.long)\n",
    "        dst = torch.tensor(self.train_df['item_idx'].values, dtype=torch.long) + self.num_users\n",
    "        \n",
    "        # Tạo edge_index vô hướng (2 chiều: user->item và item->user)\n",
    "        self.edge_index = torch.stack([torch.cat([src, dst]), torch.cat([dst, src])], dim=0)\n",
    "\n",
    "        # --- 4. Prepare User History (Cho việc sampling/evaluation nếu cần) ---\n",
    "        # Dùng set để tra cứu nhanh O(1)\n",
    "        self.train_user_pos_items = self._build_user_history(self.train_df)\n",
    "        self.val_user_pos_items = self._build_user_history(self.val_df)\n",
    "        self.test_user_pos_items = self._build_user_history(self.test_df)\n",
    "\n",
    "    def _build_user_history(self, df_subset):\n",
    "        \"\"\"Hàm phụ trợ để gom nhóm item theo user\"\"\"\n",
    "        user_pos_items = defaultdict(set)\n",
    "        # Zip nhanh hơn iterrows\n",
    "        for u, i in zip(df_subset['user_idx'], df_subset['item_idx']):\n",
    "            user_pos_items[u].add(i)\n",
    "        return user_pos_items\n",
    "\n",
    "    def _create_dataloader(self, df_subset, shuffle):\n",
    "        # Chuyển đổi thành TensorDataset để DataLoader hiểu\n",
    "        users = torch.tensor(df_subset['user_idx'].values, dtype=torch.long)\n",
    "        items = torch.tensor(df_subset['item_idx'].values, dtype=torch.long)\n",
    "        dataset = TensorDataset(users, items)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Shuffle=True cho Train set\n",
    "        return self._create_dataloader(self.train_df, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._create_dataloader(self.val_df, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._create_dataloader(self.test_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-L86-kxBGia"
   },
   "outputs": [],
   "source": [
    "class TGCNRecommender(pl.LightningModule):\n",
    "    def __init__(self, num_cells, num_users, num_items, batch_size, embedding_dim, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Important: We use manual optimization to handle mini-batches of interactions\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.num_cells = num_cells\n",
    "\n",
    "        self.num_nodes = num_users + num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Learnable Node Embeddings\n",
    "        self.node_emb = nn.Embedding(self.num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.node_emb.weight)\n",
    "\n",
    "        # T-GCN Layer\n",
    "        self.tgcns = nn.ModuleList([TGCN(in_channels=embedding_dim, \n",
    "                                        out_channels=embedding_dim) for _ in range(num_cells)])\n",
    "\n",
    "        self.lr = lr\n",
    "        self.h0 = None\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.h0 = None\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.h0 = None\n",
    "\n",
    "    def forward(self, edge_index, target_u, target_i, h):\n",
    "        # 1. Get current node embeddings\n",
    "        x = self.node_emb.weight\n",
    "\n",
    "        # 2. Update Embeddings with T-GCN\n",
    "        h_out = self.h0\n",
    "        for tgcn in self.tgcns:\n",
    "            h_out = tgcn(x, self.edge_index, h_out) #h_out shape: [num_nodes, embedding_dim]\n",
    "\n",
    "        user_embs = h_out[:self.num_users]\n",
    "        item_embs = h_out[self.num_users:]\n",
    "\n",
    "        return user_embs, item_embs\n",
    "\n",
    "    def compute_loss(self, batch, user_embs, item_embs):\n",
    "        user_ids, item_ids = batch\n",
    "        pos_item_ids = item_ids - self.hparams.num_users\n",
    "\n",
    "        # Get embeddings\n",
    "        user_emb = full_user_embs[user_ids]\n",
    "        pos_emb = full_item_embs[pos_item_ids]\n",
    "\n",
    "        # Compute positive scores\n",
    "        pos_scores = torch.exp(-torch.abs(user_emb - pos_emb).sum(dim=1))\n",
    "\n",
    "        ####################### Hard negative Sampling #######################\n",
    "        distances = torch.cdist(user_emb, full_item_embs, p=1)\n",
    "        scores = torch.exp(-distances)\n",
    "\n",
    "        ######## Mask all pos_item_ids of the user in train_dataset ########\n",
    "        ### Basically, the  model should only see the information in the train_dataset.\n",
    "        ### Therefore, only mask the pos_item_ids of the user in train_dataset\n",
    "        ### All cell (user, item) in val_dataset should be treated as blank hence don't mask the val_dataset\n",
    "\n",
    "        for i, u in enumerate(user_ids.tolist()):\n",
    "            pos_item_ids = [item - self.num_users for item in self.train_user_pos_items[u]]\n",
    "            scores[i, pos_item_ids] = float('-inf')\n",
    "        ######## Mask all pos_item_ids of the user in train_dataset ########\n",
    "\n",
    "        k = 10 # Select top-K most negatives for each user\n",
    "        neg_item_ids = torch.topk(scores, k=k, dim=1).indices\n",
    "\n",
    "        # Get embeddings for these negatives\n",
    "        neg_emb = full_item_embs[neg_item_ids]\n",
    "\n",
    "        neg_scores = torch.exp(-torch.abs(user_emb.unsqueeze(1) - neg_emb).sum(dim=2))\n",
    "        neg_scores = neg_scores.mean(dim=1)\n",
    "        ####################### Hard negative Sampling #######################\n",
    "\n",
    "\n",
    "        ####################### Compute Loss #######################\n",
    "        scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
    "        labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)], dim=0)\n",
    "\n",
    "        loss = F.binary_cross_entropy(scores, labels)\n",
    "        ####################### Compute Loss #######################\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_embs, item_embs = self()\n",
    "        loss = self.compute_loss(batch, user_embs, item_embs)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRuOnaapBK81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes -> Train: 69843, Val: 36616, Test: 131\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m data_module.prepare_data()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Model Init with Interaction Batch Size\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m embedding_dim = \u001b[32;43m64\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# We set interaction_batch_size to process ratings in chunks\u001b[39;00m\n\u001b[32m      8\u001b[39m model = TGCNRecommender(\n\u001b[32m      9\u001b[39m     num_users=num_users,\n\u001b[32m     10\u001b[39m     num_items=num_items,\n\u001b[32m     11\u001b[39m     embedding_dim=embedding_dim,\n\u001b[32m     12\u001b[39m     interaction_batch_size=\u001b[32m1024\u001b[39m \u001b[38;5;66;03m# Adjust this based on GPU memory\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m data_module.prepare_data()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Model Init with Interaction Batch Size\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m embedding_dim = \u001b[32;43m64\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# We set interaction_batch_size to process ratings in chunks\u001b[39;00m\n\u001b[32m      8\u001b[39m model = TGCNRecommender(\n\u001b[32m      9\u001b[39m     num_users=num_users,\n\u001b[32m     10\u001b[39m     num_items=num_items,\n\u001b[32m     11\u001b[39m     embedding_dim=embedding_dim,\n\u001b[32m     12\u001b[39m     interaction_batch_size=\u001b[32m1024\u001b[39m \u001b[38;5;66;03m# Adjust this based on GPU memory\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rs_env/lib/python3.11/threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_module = DataModule('data/book_interaction.csv')\n",
    "    data_module.prepare_data()\n",
    "\n",
    "    model = TGCNRecommender(\n",
    "        num_cells = 3,\n",
    "        num_users=data_module.num_users,\n",
    "        num_items=data_module.num_items,\n",
    "        batch_size = 1024,\n",
    "        embedding_dim= 64,\n",
    "        lr = 0.001,\n",
    "    )\n",
    "\n",
    "    # DataLoaders - Keep batch_size=1 to load one temporal snapshot at a time\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=lambda x: x[0], shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=lambda x: x[0], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=lambda x: x[0], shuffle=False)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        # log_every_n_steps=1 might be too frequent for inner loops, but okay here\n",
    "        log_every_n_steps=1\n",
    "    )\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    print(\"Training Complete!\")\n",
    "\n",
    "    print(\"Starting Testing...\")\n",
    "    trainer.test(model, test_loader)\n",
    "    print(\"Testing Complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqa2xQZFwDaTImHhVca8F7",
   "gpuType": "T4",
   "mount_file_id": "1xEeIB3zDaFVJwvNavB_5aFrzl6IqQUC5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
