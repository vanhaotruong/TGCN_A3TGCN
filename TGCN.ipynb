{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-5Jpgg49wzb",
    "outputId": "ec14c399-bd0d-4523-cdbc-4084ddfbfcfb"
   },
   "outputs": [],
   "source": [
    "# # 1. Gỡ bỏ phiên bản quá mới hiện tại\n",
    "# !pip uninstall torch torchvision torchaudio torch-scatter torch-sparse torch-geometric torch-geometric-temporal -y\n",
    "\n",
    "# # 2. Cài đặt PyTorch 2.5.1 (Bản ổn định) + CUDA 12.4\n",
    "# !pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "# # 3. Cài đặt các thư viện vệ tinh (Scatter/Sparse) dành RIÊNG cho bản 2.5.1\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
    "\n",
    "# # 4. Cài thư viện chính\n",
    "# !pip install pytorch_lightning torch-geometric torch-geometric-temporal\n",
    "\n",
    "# # # 5. Runtime > Restart session\n",
    "# # # 6 Ignore this !pip section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 6172,
     "status": "error",
     "timestamp": 1768380494093,
     "user": {
      "displayName": "van hao truong",
      "userId": "08998182992862504611"
     },
     "user_tz": -420
    },
    "id": "xDk5XgSY3-_w",
    "outputId": "4b24502f-92da-42d7-8a99-05db6574f795"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric_temporal.nn.recurrent import TGCN\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set environment variables for reproducibility and safety\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "# 1. Configuration & Seeding\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrrEyokS4NK9"
   },
   "outputs": [],
   "source": [
    "def load_tgcn_data(data_dir='data'):\n",
    "    print(\"Loading interactions...\")\n",
    "    # Load and process interaction data\n",
    "    # Only using book_interaction.csv\n",
    "    file_path = os.path.join(data_dir, 'book_interaction.csv')\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    df_inter = pd.read_csv(file_path)\n",
    "    # Clean column names (strip type suffix if present, e.g. user_id:token -> user_id)\n",
    "    df_inter.columns = [c.split(':')[0] for c in df_inter.columns]\n",
    "\n",
    "    # Ensure timestamp is datetime\n",
    "    df_inter['timestamp'] = pd.to_datetime(df_inter['timestamp'])\n",
    "\n",
    "    print(\"Mapping IDs...\")\n",
    "    user_encoder = LabelEncoder()\n",
    "    item_encoder = LabelEncoder()\n",
    "\n",
    "    # Encode Users and Items\n",
    "    df_inter['user_idx'] = user_encoder.fit_transform(df_inter['user_id'].astype(str))\n",
    "    df_inter['item_idx'] = item_encoder.fit_transform(df_inter['item_id'].astype(str))\n",
    "\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_items = len(item_encoder.classes_)\n",
    "\n",
    "    print(f\"Total Users: {num_users}, Total Items: {num_items}\")\n",
    "\n",
    "    print(\"Creating temporal snapshots...\")\n",
    "    df_inter['month'] = df_inter['timestamp'].dt.to_period('M')\n",
    "    # Sort by month to ensure temporal order\n",
    "    months = sorted(df_inter['month'].unique())\n",
    "\n",
    "    # 1. Determine Training Split (70%) to build the static graph\n",
    "    train_len_months = int(len(months) * 0.7)\n",
    "    train_months = months[:train_len_months]\n",
    "\n",
    "    # 2. Get all interactions in the training set\n",
    "    df_train = df_inter[df_inter['month'].isin(train_months)]\n",
    "\n",
    "    # 3. Build Static Edge Index from Train Set\n",
    "    train_u_idx = df_train['user_idx'].values\n",
    "    train_i_idx = df_train['item_idx'].values\n",
    "\n",
    "    train_u_node_idx = torch.tensor(train_u_idx, dtype=torch.long)\n",
    "    train_i_node_idx = torch.tensor(train_i_idx + num_users, dtype=torch.long)\n",
    "\n",
    "    # Create undirected graph from unique training interactions\n",
    "    # Note: torch.unique might be needed if multiple interactions exist, but edge_index usually works fine with multis.\n",
    "    # We'll use the raw list; duplicates increase weight in message passing or are redundant.\n",
    "    # For efficiency and cleanliness, let's keep unique edges.\n",
    "    train_edges_df = df_train[['user_idx', 'item_idx']].drop_duplicates()\n",
    "    unique_u_idx = torch.tensor(train_edges_df['user_idx'].values, dtype=torch.long)\n",
    "    unique_i_idx = torch.tensor(train_edges_df['item_idx'].values + num_users, dtype=torch.long)\n",
    "\n",
    "    train_edge_index = torch.stack([\n",
    "        torch.cat([unique_u_idx, unique_i_idx]),\n",
    "        torch.cat([unique_i_idx, unique_u_idx])\n",
    "    ], dim=0)\n",
    "\n",
    "    print(f\"Static Training Graph created with {train_edges_df.shape[0]} edges.\")\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for m in months:\n",
    "        snapshot_df = df_inter[df_inter['month'] == m]\n",
    "        if snapshot_df.empty:\n",
    "            continue\n",
    "\n",
    "        u_idx_raw = snapshot_df['user_idx'].values\n",
    "        i_idx_raw = snapshot_df['item_idx'].values\n",
    "\n",
    "        # Node indices for validataion/testing targets\n",
    "        u_node_idx = torch.tensor(u_idx_raw, dtype=torch.long)\n",
    "        i_node_idx = torch.tensor(i_idx_raw + num_users, dtype=torch.long)\n",
    "\n",
    "        # Use the STATIC train_edge_index for Graph Structure\n",
    "        dataset.append({\n",
    "            'edge_index': train_edge_index,\n",
    "            'y': torch.ones(len(snapshot_df), dtype=torch.float), # All interactions are positive (likes)\n",
    "            'target_u': u_node_idx,\n",
    "            'target_i': i_node_idx\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(dataset)} snapshots.\")\n",
    "\n",
    "    # Split into Train (First 70%) and Remainder (30%)\n",
    "    # Requirement: First 70% for training (temporal). Remaining 30% randomly split into Val and Test.\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(total_len * 0.7)\n",
    "\n",
    "    train_dataset = dataset[:train_len]\n",
    "    remainder_dataset = dataset[train_len:]\n",
    "\n",
    "    # Shuffle the remainder to randomize validation/test split\n",
    "    import random\n",
    "    random.shuffle(remainder_dataset)\n",
    "\n",
    "    # Allocate roughly 10% of total (1/3 of remainder) to val, and 20% (2/3 of remainder) to test\n",
    "    val_len = int(total_len * 0.1)\n",
    "\n",
    "    val_dataset = remainder_dataset[:val_len]\n",
    "    test_dataset = remainder_dataset[val_len:]\n",
    "\n",
    "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-L86-kxBGia"
   },
   "outputs": [],
   "source": [
    "class TGCNRecommender(pl.LightningModule):\n",
    "    def __init__(self, num_cells, num_users, num_items, embedding_dim=64, lr=0.01, interaction_batch_size=1024):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Important: We use manual optimization to handle mini-batches of interactions\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.num_cells = num_cells\n",
    "\n",
    "        self.num_nodes = num_users + num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Learnable Node Embeddings\n",
    "        self.node_emb = nn.Embedding(self.num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.node_emb.weight)\n",
    "\n",
    "        # T-GCN Layer\n",
    "        self.tgcns = nn.ModuleList([TGCN(in_channels=embedding_dim, \n",
    "                                        out_channels=embedding_dim) for _ in range(num_cells)])\n",
    "\n",
    "        self.lr = lr\n",
    "        self.h0 = None\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.h0 = None\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.h0 = None\n",
    "\n",
    "    def forward(self, edge_index, target_u, target_i, h):\n",
    "        # 1. Get current node embeddings\n",
    "        x = self.node_emb.weight\n",
    "\n",
    "        # 2. Update Embeddings with T-GCN\n",
    "        h_out = self.h0\n",
    "        for tgcn in self.tgcns:\n",
    "            h_out = tgcn(x, self.edge_index, h_out) #h_out shape: [num_nodes, embedding_dim]\n",
    "\n",
    "        user_embs = h_out[:self.num_users]\n",
    "        item_embs = h_out[self.num_users:]\n",
    "\n",
    "        return user_embs, item_embs\n",
    "\n",
    "    def compute_loss(self, batch, user_embs, item_embs):\n",
    "        user_ids, item_ids = batch\n",
    "        pos_item_ids = item_ids - self.hparams.num_users\n",
    "\n",
    "        # Get embeddings\n",
    "        user_emb = full_user_embs[user_ids]\n",
    "        pos_emb = full_item_embs[pos_item_ids]\n",
    "\n",
    "        # Compute positive scores\n",
    "        pos_scores = torch.exp(-torch.abs(user_emb - pos_emb).sum(dim=1))\n",
    "\n",
    "        ####################### Hard negative Sampling #######################\n",
    "        distances = torch.cdist(user_emb, full_item_embs, p=1)\n",
    "        scores = torch.exp(-distances)\n",
    "\n",
    "        ######## Mask all pos_item_ids of the user in train_dataset ########\n",
    "        ### Basically, the  model should only see the information in the train_dataset.\n",
    "        ### Therefore, only mask the pos_item_ids of the user in train_dataset\n",
    "        ### All cell (user, item) in val_dataset should be treated as blank hence don't mask the val_dataset\n",
    "\n",
    "        for i, u in enumerate(user_ids.tolist()):\n",
    "            pos_item_ids = [item - self.num_users for item in self.train_user_pos_items[u]]\n",
    "            scores[i, pos_item_ids] = float('-inf')\n",
    "        ######## Mask all pos_item_ids of the user in train_dataset ########\n",
    "\n",
    "        k = 10 # Select top-K most negatives for each user\n",
    "        neg_item_ids = torch.topk(scores, k=k, dim=1).indices\n",
    "\n",
    "        # Get embeddings for these negatives\n",
    "        neg_emb = full_item_embs[neg_item_ids]\n",
    "\n",
    "        neg_scores = torch.exp(-torch.abs(user_emb.unsqueeze(1) - neg_emb).sum(dim=2))\n",
    "        neg_scores = neg_scores.mean(dim=1)\n",
    "        ####################### Hard negative Sampling #######################\n",
    "\n",
    "\n",
    "        ####################### Compute Loss #######################\n",
    "        scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
    "        labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)], dim=0)\n",
    "\n",
    "        loss = F.binary_cross_entropy(scores, labels)\n",
    "        ####################### Compute Loss #######################\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_embs, item_embs = self()\n",
    "        loss = self.compute_loss(batch, full_user_embs, full_item_embs)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._evaluate_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._evaluate_step(batch, batch_idx, \"test\")\n",
    "\n",
    "    def _evaluate_step(self, batch, batch_idx, stage):\n",
    "        edge_index, y = batch['edge_index'], batch['y']\n",
    "        target_u, target_i = batch['target_u'], batch['target_i']\n",
    "\n",
    "        if self.h is None:\n",
    "             self.h = torch.zeros(self.num_nodes, self.embedding_dim, device=self.device)\n",
    "        else:\n",
    "             self.h = self.h.to(self.device).detach()\n",
    "\n",
    "        # Forward pass (Full Batch inference for embeddings is usually fine,\n",
    "        # but we batch the scoring to save memory)\n",
    "        x = self.node_emb.weight\n",
    "        h_new = self.tgcn(x, edge_index, None, self.h)\n",
    "        self.h = h_new.detach()\n",
    "\n",
    "        # Evaluation in chunks\n",
    "        batch_size = self.hparams.interaction_batch_size\n",
    "        num_interactions = len(target_u)\n",
    "\n",
    "        total_tp, total_fp, total_fn, total_tn = 0, 0, 0, 0\n",
    "\n",
    "        for start_idx in range(0, num_interactions, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_interactions)\n",
    "            batch_u = target_u[start_idx:end_idx]\n",
    "            batch_i = target_i[start_idx:end_idx]\n",
    "\n",
    "            # Positive Predictions\n",
    "            u_emb = h_new[batch_u]\n",
    "            i_emb = h_new[batch_i]\n",
    "            pos_preds = self.predictor(torch.cat([u_emb, i_emb], dim=1)).view(-1)\n",
    "            pos_probs = torch.sigmoid(pos_preds)\n",
    "\n",
    "            # Negative Sampling (1:1)\n",
    "            # We generate negatives on the fly for evaluation\n",
    "            neg_i = torch.randint(\n",
    "                self.hparams.num_users,\n",
    "                self.hparams.num_users + self.hparams.num_items,\n",
    "                (len(batch_u),),\n",
    "                device=self.device\n",
    "            )\n",
    "            neg_i_emb = h_new[neg_i]\n",
    "            neg_preds = self.predictor(torch.cat([u_emb, neg_i_emb], dim=1)).view(-1)\n",
    "            neg_probs = torch.sigmoid(neg_preds)\n",
    "\n",
    "            # Metrics\n",
    "            all_probs = torch.cat([pos_probs, neg_probs])\n",
    "            all_labels = torch.cat([torch.ones_like(pos_probs), torch.zeros_like(neg_probs)])\n",
    "            preds = (all_probs > 0.5).float()\n",
    "\n",
    "            total_tp += ((preds == 1) & (all_labels == 1)).sum().item()\n",
    "            total_fp += ((preds == 1) & (all_labels == 0)).sum().item()\n",
    "            total_fn += ((preds == 0) & (all_labels == 1)).sum().item()\n",
    "            total_tn += ((preds == 0) & (all_labels == 0)).sum().item()\n",
    "\n",
    "        precision = total_tp / (total_tp + total_fp + 1e-8)\n",
    "        recall = total_tp / (total_tp + total_fn + 1e-8)\n",
    "        accuracy = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn + 1e-8)\n",
    "\n",
    "        self.log_dict({\n",
    "            f\"{stage}_precision\": precision,\n",
    "            f\"{stage}_recall\": recall,\n",
    "            f\"{stage}_accuracy\": accuracy\n",
    "        }, prog_bar=True)\n",
    "\n",
    "        return precision\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRuOnaapBK81"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Reload data to ensure clean state\n",
    "    train_dataset, val_dataset, test_dataset, num_users, num_items = load_tgcn_data(\"data\")\n",
    "\n",
    "    # Model Init with Interaction Batch Size\n",
    "    embedding_dim = 64\n",
    "    # We set interaction_batch_size to process ratings in chunks\n",
    "    model = TGCNRecommender(\n",
    "        num_users=num_users,\n",
    "        num_items=num_items,\n",
    "        embedding_dim=embedding_dim,\n",
    "        interaction_batch_size=1024 # Adjust this based on GPU memory\n",
    "    )\n",
    "\n",
    "    # DataLoaders - Keep batch_size=1 to load one temporal snapshot at a time\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=lambda x: x[0], shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=lambda x: x[0], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=lambda x: x[0], shuffle=False)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        # log_every_n_steps=1 might be too frequent for inner loops, but okay here\n",
    "        log_every_n_steps=1\n",
    "    )\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    print(\"Training Complete!\")\n",
    "\n",
    "    print(\"Starting Testing...\")\n",
    "    trainer.test(model, test_loader)\n",
    "    print(\"Testing Complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqa2xQZFwDaTImHhVca8F7",
   "gpuType": "T4",
   "mount_file_id": "1xEeIB3zDaFVJwvNavB_5aFrzl6IqQUC5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
